{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3df1f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FakeNewsDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3318a320",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the dependency\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c840827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sandipan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30ba41f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#printing the stopwords in English\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "094e9946",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Pre-processing\n",
    "#loading the dataset to a pandas DataFrame\n",
    "news_dataset=pd.read_csv(\"fake_news_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a352d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4998, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3876682",
   "metadata": {},
   "outputs": [],
   "source": [
    "#coverting variable to quantity data\n",
    "vlabel=pd.get_dummies(news_dataset['label'],drop_first=True)\n",
    "news_dataset=pd.concat((news_dataset,vlabel),axis=1)\n",
    "news_dataset.drop(['label'],axis=1,inplace=True)\n",
    "news_dataset.rename(columns={'real':'label'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e0ce5c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Get the latest from TODAY Sign up for our news...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2d  Conan On The Funeral Trump Will Be Invited...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It’s safe to say that Instagram Stories has fa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Much like a certain Amazon goddess with a lass...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>At a time when the perfect outfit is just one ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           news_text  label\n",
       "0  Get the latest from TODAY Sign up for our news...      0\n",
       "1  2d  Conan On The Funeral Trump Will Be Invited...      0\n",
       "2  It’s safe to say that Instagram Stories has fa...      1\n",
       "3  Much like a certain Amazon goddess with a lass...      1\n",
       "4  At a time when the perfect outfit is just one ...      1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#printing the first 5 rows of the dataframe\n",
    "news_dataset.head()\n",
    "#here 1 indicates fake news and 0 indicates not fake news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8cfad28d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "news_text    0\n",
       "label        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counting the number of missing values in the dataset\n",
    "news_dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b432522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing the null values with empty string\n",
    "news_dataset=news_dataset.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2483754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              news_text\n",
      "0     Get the latest from TODAY Sign up for our news...\n",
      "1     2d  Conan On The Funeral Trump Will Be Invited...\n",
      "2     It’s safe to say that Instagram Stories has fa...\n",
      "3     Much like a certain Amazon goddess with a lass...\n",
      "4     At a time when the perfect outfit is just one ...\n",
      "...                                                 ...\n",
      "4993  A jury ruled against Bill Cosby in his sexual ...\n",
      "4994                An unicorn was found in city centre\n",
      "4995        An alien was spotted walking on the streets\n",
      "4996  Twitter has not taken any legal action against...\n",
      "4997  It appears that the index has embarked on a su...\n",
      "\n",
      "[4998 rows x 1 columns]\n",
      "0       0\n",
      "1       0\n",
      "2       1\n",
      "3       1\n",
      "4       1\n",
      "       ..\n",
      "4993    1\n",
      "4994    0\n",
      "4995    0\n",
      "4996    1\n",
      "4997    1\n",
      "Name: label, Length: 4998, dtype: uint8\n"
     ]
    }
   ],
   "source": [
    "#seperating the data & label\n",
    "X = news_dataset.drop(columns='label',axis=1)\n",
    "Y= news_dataset['label']\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b908b99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming:- Stemming is the process of reducing a word to its Root \n",
    "#example:- actor,actress,acting -->act\n",
    "port_stem = PorterStemmer()\n",
    "def stemming(news_text):\n",
    "    stemmed_text=re.sub('[^a-zA-Z]',' ',news_text)\n",
    "    stemmed_text=stemmed_text.lower()\n",
    "    stemmed_text=stemmed_text.split()\n",
    "    stemmed_text= [port_stem.stem(word) for word in stemmed_text if not word in stopwords.words('english')]\n",
    "    stemmed_text= ' '.join(stemmed_text)\n",
    "    return stemmed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f304f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset['news_text'] = news_dataset['news_text'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4bfe3f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating the data and label\n",
    "X=news_dataset['news_text'].values\n",
    "Y = news_dataset['label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d02f12ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['get latest today sign newslett one ever truli get lose love one blake shelton except older brother richi die nov shelton note tweet monday chang life forev richi die car accid shelton home state oklahoma two year ago shelton sent messag th anniversari loss richi blake half brother share mother passeng car collid school bu ada south oklahoma citi richi driver redena mcmanu year old boy christoph mcmanu die shortli collis bu driver passeng uninjur accord polic report accid clearli remain blake told minut rememb pick phone call week dead tell someth pick phone call tell someth saw tv like constantli shock dead blake shelton play today halloween extravaganza new york citi oct getti imag blake wife miranda lambert wrote singl call inspir richi still two brother bond despit age differ share love countri music bedroom right across hallway mine littl blake said interview listen hank william jr waylon lynyrd skynyrd bob seeger whatev popular realli richi love music would sit go man guy hero coolest guy big brother follow rande dawn twitter'\n",
      " 'conan funer trump invit conan tb'\n",
      " 'safe say instagram stori far surpass competitor snapchat popular sinc incept two year ago favorit celebr hop social media trend unlik highli curat photo feed instagram stori celebr seem comfort enough raw open need someth wait line short break take peek celebr instagram stori surprisingli engag entertain busi philipp busyphilipp fantast stori teller busi dub new yorker breakout star instagram stori captur everyth morn workout paparazzi run in everyth busi stori assum happen mandi moor mandymooremm follow mandi moor mani us behind scene stori worth alon also instastori home built decor mount kilimanjaro climb prepar behind hollywood red carpet event recent attend chrissi teigen chrissyteigen follow love twitter snapchat watch instagram stori humor cook ador daughter luna rees witherspoon reesewitherspoon rees may one biggest star world earth breath fresh air instagram stori sarah hyland sarahhyland somehow modern famili star make eat dinner solo watch bachelor interest enough keep watch candac cameron bure candacecbur soft spot candac sinc grow full hous live resurg career follow candac instagram stori fashion famili workout behind scene movi tv set eva chen evachen though may bias use platform sinc becom director fashion partnership instagram still one watch eva chronicl daili life earli morn ador kid intern fashion week event jessica alba jessicaalba new mama three ador show juggl home work life prepar back tv sure behind scene sneak peek kristin cavallari kristincavallari realiti tv cookbook author fashion design kristin share recip open first brick mortar fashion home design line uncommon jame spencer pratt spencerpratt hear hill alum may villain bad rap minut fame instagram stori highlight famili life month old son gunner wife heidi mani obsess includ hummingbird work coffe taylor swift'\n",
      " ... 'alien spot walk street'\n",
      " 'twitter taken legal action elon musk see troll deal plan carri task need complet transact'\n",
      " 'appear index embark sustain pullback ralli whose initi target appear bearish gap zone present rang regist may near term trend shall remain posit bias long index sustain level']\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e12616f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ebcb6ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4998,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12cd7891",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting textual data to numerical data\n",
    "#TfidfVectorizer=Tf-terms frequency,idf- inverse documents frequency\n",
    "#fit= feature\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(X)\n",
    "\n",
    "X= vectorizer.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9cf22530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 42987)\t0.036687095657634915\n",
      "  (0, 42866)\t0.03890145346743022\n",
      "  (0, 42593)\t0.035513346103561605\n",
      "  (0, 42521)\t0.026141771679327165\n",
      "  (0, 42129)\t0.04504218712712073\n",
      "  (0, 42055)\t0.03667094093049357\n",
      "  (0, 41882)\t0.05056312972930073\n",
      "  (0, 41704)\t0.030606019456130448\n",
      "  (0, 41615)\t0.10540773601124404\n",
      "  (0, 40115)\t0.1001512186965051\n",
      "  (0, 39601)\t0.04978496070613522\n",
      "  (0, 39592)\t0.03554291092105746\n",
      "  (0, 39545)\t0.04068890610986377\n",
      "  (0, 39520)\t0.037470012825171074\n",
      "  (0, 39318)\t0.05254586023753641\n",
      "  (0, 38746)\t0.02595676945197298\n",
      "  (0, 38718)\t0.07653339381700214\n",
      "  (0, 38104)\t0.04019229945273098\n",
      "  (0, 37917)\t0.0630993309457412\n",
      "  (0, 36473)\t0.03039537845555897\n",
      "  (0, 36268)\t0.03663869174258676\n",
      "  (0, 35709)\t0.05430841179035804\n",
      "  (0, 35563)\t0.06984333527623678\n",
      "  (0, 35092)\t0.10540773601124404\n",
      "  (0, 34958)\t0.04710751432010369\n",
      "  :\t:\n",
      "  (4997, 43333)\t0.1873800157134517\n",
      "  (4997, 42013)\t0.12475324869048751\n",
      "  (4997, 39137)\t0.15812300057816359\n",
      "  (4997, 38017)\t0.1253415554785386\n",
      "  (4997, 37668)\t0.15710438916229427\n",
      "  (4997, 37198)\t0.36458956080779603\n",
      "  (4997, 34276)\t0.20755793284729449\n",
      "  (4997, 31613)\t0.11393152066426701\n",
      "  (4997, 31490)\t0.18126073313686214\n",
      "  (4997, 30989)\t0.14830701542214902\n",
      "  (4997, 30933)\t0.17520992834795643\n",
      "  (4997, 30410)\t0.2872431398955406\n",
      "  (4997, 29964)\t0.11574882705199616\n",
      "  (4997, 29699)\t0.11774747186906094\n",
      "  (4997, 26146)\t0.13512458945051295\n",
      "  (4997, 23814)\t0.07469801133475301\n",
      "  (4997, 22458)\t0.08634055932563559\n",
      "  (4997, 21898)\t0.13022429198352345\n",
      "  (4997, 18407)\t0.1230468457095051\n",
      "  (4997, 18243)\t0.4561138135043564\n",
      "  (4997, 14321)\t0.18229478040389802\n",
      "  (4997, 11569)\t0.18126073313686214\n",
      "  (4997, 3618)\t0.23012547634627017\n",
      "  (4997, 3074)\t0.2872431398955406\n",
      "  (4997, 1627)\t0.16321483255270858\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3157636",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLITING DATA SET TO TRAINING & TEST DATA\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,stratify=Y,random_state=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80bcf9dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Traning the model: logistic Regression\n",
    "model=LogisticRegression()\n",
    "model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "02c351fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of the training data :  0.8529264632316158\n"
     ]
    }
   ],
   "source": [
    "#Evaluation #accuracy score\n",
    "#accuracy score for traning data\n",
    "X_train_prediction =model.predict(X_train)\n",
    "training_data_accuracy=accuracy_score(X_train_prediction,Y_train)\n",
    "print('Accuracy score of the training data : ',training_data_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "64eeb806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of the testing data :  0.757\n"
     ]
    }
   ],
   "source": [
    "#accuracy score for testing data\n",
    "X_test_prediction =model.predict(X_test)\n",
    "testing_data_accuracy=accuracy_score(X_test_prediction,Y_test)\n",
    "print('Accuracy score of the testing data : ',testing_data_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78b02cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "The news is Fake\n"
     ]
    }
   ],
   "source": [
    "#Making a predictive System\n",
    "X_new =X_test[5]\n",
    "\n",
    "prediction=model.predict(X_new)\n",
    "print(prediction)\n",
    "\n",
    "if(prediction[0]==0):\n",
    "  print('The news is Real')\n",
    "else:\n",
    "  print('The news is Fake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a726787f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(Y_test[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4372696d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2180a48d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
